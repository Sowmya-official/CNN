{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of YOLOv1, v2, v3, v4, v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.The main ideas of target detection\n",
    "\n",
    "Unlike the RCNN series, YOLO treats target detection as a regression problem, and directly uses a network for classification and box regression.\n",
    "\n",
    "The specific method is: divide the image into S * S grids, and each grid predicts the positions (x, y, w, h) of B bboxes, confidence (confidence is the intersection ratio), and class probability. The output dimension is S * S * (B * 5 + C), and C is the number of categories. No matter how many boxes are contained in the grid, each grid only predicts a set of class probabilities. During the test, the conditional class probability and the confidence of the prediction box are multiplied to indicate that each box contains the confidence of a certain type of object. This score can represent the category probability and prediction accuracy of the box at the same time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Overall network structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/SOrKLpe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic network model is GoogLe Net, but instead of using its inception module, it uses 1 * 1 and 3 * 3 convolutional layers alternately.\n",
    "\n",
    "Convolutional layer extraction features, fully connected layer prediction category and box position regression, a total of 24 convolutional layers, 2 fully connected layers.\n",
    "\n",
    "![](https://i.imgur.com/jlkynD8.png)\n",
    "                                \n",
    "                            24-Convolutional + 2 fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sub-network: pre-trained classification network, the input image size is 224 * 224\n",
    "First 20 convolutional layers + 1 global average pooling + 1 fully connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sub-network: target detection network, the input image size is 448 * 448\n",
    "\n",
    "First 20 convolutional layers + 4 convolutional layers + 2 fully connected + 1 fully connected (prediction category / frame position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Loss function (square sum loss function)\n",
    "\n",
    "Including 4 parts: box center position x, y loss + box width and height w, h loss + confidence loss + classification loss.\n",
    "\n",
    "![](https://i.imgur.com/MU4k39w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. YOLO1 advantages:\n",
    "\n",
    "* High speed. Seen as a regression problem, no complicated pipeline is needed.\n",
    "* Have a global understanding of the image. Use the features of the entire image to predict the bbox, instead of RCNN, only the   features of the candidate box can be used to predict the bbox.\n",
    "* The number of candidate boxes is much smaller, only 7 * 7 * 2 = 98. The RCNN has 2000 selectlive searches, which are       computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. YOLO1 disadvantages:\n",
    "\n",
    "* Each grid only predicts 2 bboxes, which limits the number of objects predicted by the model.\n",
    "* After multiple downsampling, the features used for bounding box prediction are relatively coarse features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Improvement 1: detect more kinds of targets\n",
    "\n",
    "Using the large classification data set ImageNet to expand the data types of target detection, it can detect 9,000 types of targets (YOLO1 only has 20 types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Improvement 2: Batch standardized BN\n",
    "\n",
    "Make the gradient larger and avoid the gradient disappearing\n",
    "Faster convergence and faster training\n",
    "Not applied to the entire data set, noisy, improving the generalization ability of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Improvement 3: Train the classification network with high-resolution images\n",
    "\n",
    "The input image size of the YOLO1 classification network is 224 * 224, and the input image size of the target detection network is 448 * 448. Therefore, YOLO1 needs to complete both the target detection task and the task of adapting to higher resolution images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Improvement 4: borrow RPN anchor boxes, have prior knowledge, and make prediction faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Improvement 5: Use k-mean clustering algorithm to get the prior box piror boxes of YOLO2\n",
    "\n",
    "Use the k-mean clustering algorithm to let the model automatically select the more appropriate a priori frame length and width (YOLO1 is manually specified, with a certain degree of subjectivity)\n",
    "\n",
    "Distance matrix of the custom clustering algorithm:, centroid is the box selected as the cluster center during clustering, and box is the other box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improvement 6: The predicted offset is limited to a grid range, and the model is more stable.\n",
    "\n",
    "The prediction is the offset of the center of the prediction box relative to the grid unit. The logistic is used to limit the prediction value to a range of 0-1, so that the box offset will not exceed 1 network (RPN prediction anchor box and prediction box bbox. Offset, it is possible that the offset is large, causing the model to be unstable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. The grid predicts 5 offsets for each bbox: tx, ty, tw, th, to\n",
    "\n",
    "Let the distance from the upper left corner of the grid to the upper left corner of the image be cx, cy, and the height and width of the piror bounding (template box) be ph and pw.\n",
    "\n",
    "The calculation of the prediction frame coordinates is shown in the figure:\n",
    "\n",
    "![](https://i.imgur.com/gnqytpM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Improvement 7: The passthrough layer is proposed, which is good for small target detection.\n",
    "\n",
    "The 26 * 26 * 512 feature map of the previous layer is divided into four, which are connected into four 13 * 13 * 2048 feature maps, and then connected with the 13 * 13 * 1024 feature map of the subsequent layer to obtain 13 * 13 * 3072 features Illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Improvement 8: Multi-scale input images for training\n",
    "\n",
    "    FCN network, not fixed input size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Classification network model (YOLO2's own model): Darknet-19\n",
    "\n",
    "Similar to vgg, in the end, global average pooling is used, each feature map gets 1 value, and then using full connection will have many fewer parameters.\n",
    "\n",
    "![](https://i.imgur.com/RCo53pX.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darknet19 : 19 convolutional layers +  5 pooling layers, the last global average pooling layer outputs 1000 categories (without using a fully connected layer).\n",
    "\n",
    "![](https://i.imgur.com/kdRDIyd.png)\n",
    "\n",
    "                      Darknet19 : 19 convolutional layers +  5 pooling layers\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.Target detection network model\n",
    "\n",
    "Remove the last 1000 class output convolutional layers of the classification network, plus 3 3 * 3 convolutional layers, each 1 * 1 convolutional layer after each 3 * 3, and the last 3 * 3 * 512 Add a passthrough layer between the second and the penultimate 3 * 3 * 1024 to get more detailed results, and the last 1 * 1 layer outputs the result. The network structure diagram is omitted. (It looks like 11 new floors are added here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Hybrid classification and detection data sets, joint training classification, detection network\n",
    "\n",
    "YOLO2 proposes a joint training mechanism that mixes images from detection and classification datasets for training. When the network sees the image marked for detection, it back-propagates based on the full yolov2 loss function. When it sees a classified image, it only back-propagates the loss from the classification-specific part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Multi-label detection\n",
    "\n",
    "There may be multiple categories of objects in each box, and softmax can only be used for single classification, so instead of sigmoid, sigmoid can be used for multi-label classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Combine the characteristics of different convolutional layers to make multi-scale predictions\n",
    "\n",
    "The feature map sampled on the current layer is added to the feature map of the upper layer to obtain a combined feature map, and some convolutional layers are added to process the combined feature map, so that a more fine-grained target can be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Network structure (DarkNet53 = DarkNet19 + ResNet)\n",
    "\n",
    "* Combine the residual thought to extract deeper semantic information.\n",
    "* Continuous 3 × 3 and 1 × 1 convolutional layers are still used.\n",
    "* Prediction is performed on three different scales by upsampling. For example, the 8 * 8 feature map upsampling and the 16 * 16 feature map are added and calculated again, so that smaller objects can be predicted.\n",
    "* A convolutional layer with a step size of 2 is used instead of the pooling layer because the pooling layer will lose information.\n",
    "\n",
    "![](https://i.imgur.com/xKsSWg9.png)\n",
    "\n",
    "![](https://i.imgur.com/3lRRtAt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Predict more goals\n",
    "\n",
    "Use k-mean mean clustering algorithm to predict 9 template boxes for each grid, so that the recall can be improved (5 in YOLO2 and 2 in YOLO1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Loss function\n",
    "\n",
    "Class prediction using cross-entropy loss function (square error for YOLO2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO v4\n",
    "\n",
    "The 4th generation of YOLO has been released in April 2020.\n",
    "\n",
    "YOLO v4 takes the influence of state of art BoF (bag of freebies) and several BoS (bag of specials). The BoF improve the accuracy of the detector, without increasing the inference time. They only increase the training cost. On the other hand, the BoS increase the inference cost by a small amount however they significantly improve the accuracy of object detection.\n",
    "\n",
    "YOLO v4 also based on the Darknet and has obtained an AP value of 43.5 percent on the COCO dataset along with a real-time speed of 65 FPS on the Tesla V100, beating the fastest and most accurate detectors in terms of both speed and accuracy. When compared with YOLO v3, the AP and FPS have increased by 10 percent and 12 percent, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO v5\n",
    "\n",
    "After the release of YOLO v4, within just two months of period, an another version of YOLO has been released called YOLO v5 ! It is by the Glenn Jocher, who already known among the community for creating the popular PyTorch implementation of YOLO v3.\n",
    "\n",
    "YOLO v5 is different from all other prior releases, as this is a PyTorch implementation rather than a fork from original Darknet. Same as YOLO v4, the YOLO v5 has a CSP backbone and PA-NET neck. The major improvements includes mosaic data augmentation and auto learning bounding box anchors.\n",
    "\n",
    "Followings are some quotes by Joseph Nelson and Jacob Solawetz.\n",
    "\n",
    "Running a Tesla P100, we saw inference times up to 0.007 seconds per image, meaning 140 frames per second (FPS)! By contrast, YOLO v4 achieved 50 FPS after having been converted to the same Ultralytics PyTorch library.\n",
    "\n",
    "YOLO v5 is small. Specifically, a weights file for YOLO v5 is 27 megabytes. Our weights file for YOLO v4 (with Darknet architecture) is 244 megabytes. YOLO v5 is nearly 90 percent smaller than YOLO v4.\n",
    "\n",
    "So, it said to be that YOLO v5 is extremely fast and lightweight than YOLO v4, while the accuracy is on par with the YOLO v4 benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
